#!/usr/bin/env python
"""
Consultor de Estilo Virtual - CLI Interface
==========================================

Interface de linha de comando para executar as principais funcionalidades do projeto.

Usage:
    python run.py --help
    python run.py collect-data
    python run.py process-data
    python run.py analyze-data
    python run.py run-all
"""

import argparse
import sys
import os
from pathlib import Path

# Adicionar src ao path
sys.path.append('src')

from data.collect_data import DataCollector
from data.process_data import DataProcessor, process_all_data

def collect_data():
    """Coleta e organiza os dados de exemplo."""
    print("ğŸ”§ Iniciando coleta de dados...")
    
    collector = DataCollector()
    
    # Mostrar informaÃ§Ãµes dos datasets
    datasets = collector.download_datasets_info()
    print("\nğŸ“Š Datasets disponÃ­veis:")
    for name, info in datasets.items():
        print(f"   ğŸ“‹ {info['name']}")
        print(f"      Tamanho: {info['size']}")
        print(f"      URL: {info['url']}")
    
    # Criar dados de exemplo
    collector.create_sample_data()
    
    # Verificar dados criados
    articles, customers, fit_data = collector.load_sample_data()
    print(f"\nâœ… Dados coletados:")
    print(f"   ğŸ‘” Artigos H&M: {len(articles)} registros")
    print(f"   ğŸ‘¥ Clientes H&M: {len(customers)} registros")
    print(f"   ğŸ“ Dados de caimento: {len(fit_data)} registros")
    
    print("\nğŸ“ Para datasets completos, consulte:")
    print(collector.get_dataset_instructions())

def process_data():
    """Processa e limpa os dados coletados."""
    print("ğŸ”„ Iniciando processamento de dados...")
    
    try:
        process_all_data()
        print("\nâœ… Processamento concluÃ­do com sucesso!")
        print("ğŸ“ Arquivos gerados em data/processed/")
        
        # Listar arquivos processados
        processed_dir = Path("data/processed")
        if processed_dir.exists():
            files = list(processed_dir.glob("*.csv"))
            print(f"\nğŸ“‹ Arquivos processados ({len(files)}):")
            for file in files:
                size_kb = file.stat().st_size / 1024
                print(f"   ğŸ“„ {file.name} ({size_kb:.1f} KB)")
        
    except FileNotFoundError:
        print("âŒ Dados brutos nÃ£o encontrados.")
        print("Execute primeiro: python run.py collect-data")

def analyze_data():
    """Executa anÃ¡lise bÃ¡sica dos dados."""
    print("ğŸ“Š Iniciando anÃ¡lise de dados...")
    
    try:
        import pandas as pd
        
        # Carregar dados hÃ­bridos
        hybrid_path = "data/processed/hybrid_dataset.csv"
        if not os.path.exists(hybrid_path):
            print("âŒ Dataset hÃ­brido nÃ£o encontrado.")
            print("Execute primeiro: python run.py process-data")
            return
        
        df = pd.read_csv(hybrid_path)
        
        print(f"\nğŸ“‹ Dataset HÃ­brido - {len(df)} registros:")
        print(f"   ğŸ‘¥ Clientes Ãºnicos: {df['customer_id'].nunique()}")
        print(f"   ğŸ‘” Produtos Ãºnicos: {df['article_id'].nunique()}")
        
        print(f"\nğŸ“Š DistribuiÃ§Ã£o por categoria:")
        category_counts = df['product_category'].value_counts()
        for category, count in category_counts.items():
            percentage = (count / len(df)) * 100
            print(f"   {category}: {count} ({percentage:.1f}%)")
        
        print(f"\nğŸ“ DistribuiÃ§Ã£o de tamanhos:")
        size_counts = df['size_recommendation'].value_counts()
        for size, count in size_counts.items():
            percentage = (count / len(df)) * 100
            print(f"   Tamanho {size}: {count} ({percentage:.1f}%)")
        
        print(f"\nâœ… DistribuiÃ§Ã£o de caimento:")
        fit_counts = df['predicted_fit'].value_counts()
        for fit, count in fit_counts.items():
            percentage = (count / len(df)) * 100
            print(f"   {fit.title()}: {count} ({percentage:.1f}%)")
            
    except ImportError:
        print("âŒ Pandas nÃ£o encontrado. Instale com: pip install pandas")

def run_all():
    """Executa todo o pipeline completo."""
    print("ğŸš€ Executando pipeline completo...\n")
    
    print("1ï¸âƒ£ Coleta de dados:")
    collect_data()
    
    print("\n2ï¸âƒ£ Processamento de dados:")
    process_data()
    
    print("\n3ï¸âƒ£ AnÃ¡lise de dados:")
    analyze_data()
    
    print("\nğŸ‰ Pipeline completo executado com sucesso!")
    print("ğŸ““ Para anÃ¡lise detalhada, execute os notebooks em notebooks/")

def main():
    parser = argparse.ArgumentParser(
        description="Consultor de Estilo Virtual - Sistema de RecomendaÃ§Ã£o de Tamanhos",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python run.py collect-data     # Coleta dados de exemplo
  python run.py process-data     # Processa e limpa dados
  python run.py analyze-data     # AnÃ¡lise bÃ¡sica dos dados
  python run.py run-all          # Executa pipeline completo

Para anÃ¡lise detalhada:
  jupyter notebook               # Execute os notebooks em notebooks/
        """
    )
    
    parser.add_argument(
        'command', 
        choices=['collect-data', 'process-data', 'analyze-data', 'run-all'],
        help='Comando a ser executado'
    )
    
    args = parser.parse_args()
    
    # Garantir que diretÃ³rios existem
    os.makedirs("data/raw", exist_ok=True)
    os.makedirs("data/processed", exist_ok=True)
    
    if args.command == 'collect-data':
        collect_data()
    elif args.command == 'process-data':
        process_data()
    elif args.command == 'analyze-data':
        analyze_data()
    elif args.command == 'run-all':
        run_all()

if __name__ == '__main__':
    main()